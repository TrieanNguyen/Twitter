{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf3e53c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database\n",
      "Connected to database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/29 12:22:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/11/29 12:22:03 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+------+\n",
      "|create_at|link|text|source|\n",
      "+---------+----+----+------+\n",
      "+---------+----+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+------+-----+\n",
      "|create_at|link|text|source|score|\n",
      "+---------+----+----+------+-----+\n",
      "+---------+----+----+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+------+-----+---+\n",
      "|create_at|link|text|source|score|who|\n",
      "+---------+----+----+------+-----+---+\n",
      "+---------+----+----+------+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+------+-----+---+\n",
      "|create_at|link|text|source|score|who|\n",
      "+---------+----+----+------+-----+---+\n",
      "+---------+----+----+------+-----+---+\n",
      "\n",
      "+---------+----+----+------+-----+---+------------+\n",
      "|create_at|link|text|source|score|who|media_source|\n",
      "+---------+----+----+------+-----+---+------------+\n",
      "+---------+----+----+------+-----+---+------------+\n",
      "\n",
      "+---------+----+----+------+-----+---+------------+\n",
      "|create_at|link|text|source|score|who|media_source|\n",
      "+---------+----+----+------+-----+---+------------+\n",
      "+---------+----+----+------+-----+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, TimestampType\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "TOPIC='twitter'\n",
    "DATABASE = 'twitter'\n",
    "USERNAME = 'root'\n",
    "PASSWORD = '123'\n",
    "\n",
    "print(\"Connecting to the database\")\n",
    "try:\n",
    "    connection = mysql.connector.connect(host='172.16.0.5', database=DATABASE, user=USERNAME, password=PASSWORD)\n",
    "except Exception:\n",
    "    print(\"Could not connect to database. Please check credentials\")\n",
    "else:\n",
    "    print(\"Connected to database\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"transform\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "def sentiment_score(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "udf_sentiment_score = udf(sentiment_score, IntegerType())\n",
    "\n",
    "def who(text):\n",
    "    text_lower = text.lower()\n",
    "    print(text_lower)\n",
    "    ronaldo = 0\n",
    "    messi = 0\n",
    "    #ronaldo, #messi\n",
    "    if(('ronaldo') in text_lower) or (('cr7') in text_lower):\n",
    "        ronaldo = 1\n",
    "    if(('messi') in text_lower) or(('m10') in text_lower):\n",
    "        messi = 1\n",
    "    if(ronaldo == messi):\n",
    "        if(ronaldo == 1):\n",
    "            return 'both'\n",
    "        else:\n",
    "            return 'another'\n",
    "    elif(messi==1):\n",
    "        return 'messi'\n",
    "    else:\n",
    "        return 'ronaldo'\n",
    "udf_who = udf(who, StringType())\n",
    "\n",
    "def source(text):\n",
    "    return text.split(\">\")[1].split(\"<\")[0]\n",
    "udf_source = udf(source, StringType())\n",
    "\n",
    "date_time = datetime.today().replace(second = 0) + timedelta(hours = 7) - timedelta(minutes = 5)\n",
    "\n",
    "now = (date_time).strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "now_add_1 = (date_time + timedelta(minutes = 6)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "sql = \"select create_at, link, text, source from tweet where create_at > \" + \"'\" + now + \"'\" + \" and create_at <= \" + \"'\" + now_add_1 + \"'\"\n",
    "pdf = pd.read_sql(sql, con=connection)\n",
    "schema = StructType([ \\\n",
    "    StructField(\"create_at\",StringType(),True), \\\n",
    "    StructField(\"link\",StringType(),True), \\\n",
    "    StructField(\"text\",StringType(),True), \\\n",
    "    StructField(\"source\",StringType(),True)\n",
    "                    ])\n",
    "df = spark.createDataFrame(pdf,schema=schema)\n",
    "\n",
    "#df.show()\n",
    "df_sentiment_score = df.withColumn('score', udf_sentiment_score('text'))\n",
    "\n",
    "#df_sentiment_score.show()\n",
    "\n",
    "df_who = df_sentiment_score.withColumn('who', udf_who('text'))\n",
    "\n",
    "#df_who.show()\n",
    "\n",
    "df_who_filter = df_who.filter(\"who like 'messi' or who like 'ronaldo'\")\n",
    "\n",
    "#df_who_filter.show()\n",
    "\n",
    "df_source = df_who_filter.withColumn('media_source', udf_source('source'))\n",
    "\n",
    "#df_source.show()\n",
    "\n",
    "path = (date_time).strftime(\"%Y/%m/%d/%H/%M\")\n",
    "\n",
    "#df_source.write.parquet(\"hdfs://master:9000/data/\" + path)\n",
    "\n",
    "df_source = df_source.withColumn(\"create_at\", (df_source.create_at.cast(\"timestamp\")))\n",
    "\n",
    "df_source.show()\n",
    "\n",
    "df_source.write.mode(\"append\").saveAsTable(\"twitter.minutes\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "972a3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a4597fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b29af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(host='172.16.0.5', database=DATABASE, user=USERNAME, password=PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apt install -y python3-mysql.connector\n",
    "pip3 install pandas\n",
    "pip3 install findspark\n",
    "pip3 install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c99136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 11, 23, 0, 46, 0, 830338)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().replace(second=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b773ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master(\"spark://master:7077\").getOrCreate()\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"x\")\\\n",
    "    .config(\"spark.jars\", \"./mysql-connector-java-8.0.27.jar\")\\\n",
    "    .config(\"spark.submit.deployMode\", \"cluster\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"./mysql-connector-java-8.0.27.jar\")\\\n",
    "    .config(\"spark.jars.packages\", \"./mysql-connector-java-8.0.27.jar\")\\\n",
    "    .getOrCreate()\n",
    "    #.config(\"spark.driver.extraClassPath\", \"/home/tuhin/mysql.jar\")\\\n",
    "\n",
    "# SUBMIT_ARGS = \"--packages mysql:mysql-connector-java:5.1.39 pyspark-shell\"\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "    \n",
    "# dataframe_mysql = spark.read.format(\"jdbc\").options(\n",
    "#     url=\"jdbc:mysql://loclahost:3306/twitter\",\n",
    "#     driver = \"com.mysql.jdbc.Driver\",\n",
    "#     dbtable = \"tweet\",\n",
    "#     user=\"root\",\n",
    "#     password=\"123\").load()\n",
    "\n",
    "# dataframe_mysql.show()    \n",
    "\n",
    "# dataframe_mysql = spark.read\\\n",
    "#     .format(\"jdbc\")\\\n",
    "#     .option(\"url\", \"jdbc:mysql://172.16.0.5/twitter\")\\\n",
    "#     .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "#     .option(\"dbtable\", \"tweet\").option(\"user\", \"root\")\\\n",
    "#     .option(\"password\", \"123\").load()\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f46225ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "573c4da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1482.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105345/1892847478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataframe_mysql = spark.read.format(\"jdbc\").options(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jdbc:mysql://172.16.0.5:3306/twitter\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"com.mysql.jdbc.Driver\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdbtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1482.load.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)\n\tat sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "dataframe_mysql = spark.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://172.16.0.5:3306/twitter\",\n",
    "    driver = \"com.mysql.jdbc.Driver\",\n",
    "    dbtable = \"tweet\",\n",
    "    user=\"root\",\n",
    "    password=\"123\").load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
